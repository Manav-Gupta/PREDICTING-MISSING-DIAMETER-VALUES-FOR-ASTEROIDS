{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Asteroid-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manav-Gupta/asteroid-LSTM/blob/main/Asteroid_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v37GPr0xQ0sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11ac8f4-009f-4aad-de6f-619a307121e5"
      },
      "source": [
        "# mounting the drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz53BgcyQ2Cn"
      },
      "source": [
        "# importing all the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import keras\n",
        "from pandas import DataFrame\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adam\n",
        "from datetime import datetime\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvUYj-R7Rfg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "5afb756c-b6de-4c15-9c06-54b21536d9d5"
      },
      "source": [
        "# reading the dataset\n",
        "dataset=pd.read_csv('drive/My Drive/results.csv')\n",
        "dataset.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10,26,33,40,41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>full_name</th>\n",
              "      <th>a</th>\n",
              "      <th>e</th>\n",
              "      <th>i</th>\n",
              "      <th>om</th>\n",
              "      <th>w</th>\n",
              "      <th>q</th>\n",
              "      <th>ad</th>\n",
              "      <th>per_y</th>\n",
              "      <th>data_arc</th>\n",
              "      <th>condition_code</th>\n",
              "      <th>n_obs_used</th>\n",
              "      <th>n_del_obs_used</th>\n",
              "      <th>n_dop_obs_used</th>\n",
              "      <th>H</th>\n",
              "      <th>neo</th>\n",
              "      <th>full_name.1</th>\n",
              "      <th>a.1</th>\n",
              "      <th>e.1</th>\n",
              "      <th>i.1</th>\n",
              "      <th>om.1</th>\n",
              "      <th>w.1</th>\n",
              "      <th>q.1</th>\n",
              "      <th>ad.1</th>\n",
              "      <th>per_y.1</th>\n",
              "      <th>data_arc.1</th>\n",
              "      <th>condition_code.1</th>\n",
              "      <th>n_obs_used.1</th>\n",
              "      <th>n_del_obs_used.1</th>\n",
              "      <th>n_dop_obs_used.1</th>\n",
              "      <th>H.1</th>\n",
              "      <th>pha</th>\n",
              "      <th>diameter</th>\n",
              "      <th>extent</th>\n",
              "      <th>albedo</th>\n",
              "      <th>rot_per</th>\n",
              "      <th>GM</th>\n",
              "      <th>BV</th>\n",
              "      <th>UB</th>\n",
              "      <th>IR</th>\n",
              "      <th>spec_B</th>\n",
              "      <th>spec_T</th>\n",
              "      <th>G</th>\n",
              "      <th>moid</th>\n",
              "      <th>class</th>\n",
              "      <th>n</th>\n",
              "      <th>ma</th>\n",
              "      <th>diameter_sigma</th>\n",
              "      <th>per</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1 Ceres (A801 AA)</td>\n",
              "      <td>2.769165</td>\n",
              "      <td>0.076009</td>\n",
              "      <td>10.594067</td>\n",
              "      <td>80.305531</td>\n",
              "      <td>73.597695</td>\n",
              "      <td>2.558684</td>\n",
              "      <td>2.979647</td>\n",
              "      <td>4.608202</td>\n",
              "      <td>9242.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1030</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.40</td>\n",
              "      <td>N</td>\n",
              "      <td>1 Ceres (A801 AA)</td>\n",
              "      <td>2.769165</td>\n",
              "      <td>0.076009</td>\n",
              "      <td>10.594067</td>\n",
              "      <td>80.305531</td>\n",
              "      <td>73.597695</td>\n",
              "      <td>2.558684</td>\n",
              "      <td>2.979647</td>\n",
              "      <td>4.608202</td>\n",
              "      <td>9242.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1030</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.40</td>\n",
              "      <td>N</td>\n",
              "      <td>939.400</td>\n",
              "      <td>964.4 x 964.2 x 891.8</td>\n",
              "      <td>0.0900</td>\n",
              "      <td>9.074170</td>\n",
              "      <td>62.6284</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.426</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "      <td>G</td>\n",
              "      <td>0.12</td>\n",
              "      <td>1.59478</td>\n",
              "      <td>MBA</td>\n",
              "      <td>0.213885</td>\n",
              "      <td>77.372098</td>\n",
              "      <td>0.200</td>\n",
              "      <td>1683.145703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 Pallas (A802 FA)</td>\n",
              "      <td>2.773841</td>\n",
              "      <td>0.229972</td>\n",
              "      <td>34.832931</td>\n",
              "      <td>173.024741</td>\n",
              "      <td>310.202392</td>\n",
              "      <td>2.135935</td>\n",
              "      <td>3.411748</td>\n",
              "      <td>4.619880</td>\n",
              "      <td>78910.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.20</td>\n",
              "      <td>N</td>\n",
              "      <td>2 Pallas (A802 FA)</td>\n",
              "      <td>2.773841</td>\n",
              "      <td>0.229972</td>\n",
              "      <td>34.832931</td>\n",
              "      <td>173.024741</td>\n",
              "      <td>310.202392</td>\n",
              "      <td>2.135935</td>\n",
              "      <td>3.411748</td>\n",
              "      <td>4.619880</td>\n",
              "      <td>78910.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8516</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.20</td>\n",
              "      <td>N</td>\n",
              "      <td>545.000</td>\n",
              "      <td>582x556x500</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>7.813200</td>\n",
              "      <td>14.3000</td>\n",
              "      <td>0.635</td>\n",
              "      <td>0.284</td>\n",
              "      <td>NaN</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.23429</td>\n",
              "      <td>MBA</td>\n",
              "      <td>0.213345</td>\n",
              "      <td>144.975676</td>\n",
              "      <td>18.000</td>\n",
              "      <td>1687.410992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 Juno (A804 RA)</td>\n",
              "      <td>2.668285</td>\n",
              "      <td>0.256936</td>\n",
              "      <td>12.991043</td>\n",
              "      <td>169.851484</td>\n",
              "      <td>248.066191</td>\n",
              "      <td>1.982706</td>\n",
              "      <td>3.353865</td>\n",
              "      <td>4.358696</td>\n",
              "      <td>78858.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7240</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.33</td>\n",
              "      <td>N</td>\n",
              "      <td>3 Juno (A804 RA)</td>\n",
              "      <td>2.668285</td>\n",
              "      <td>0.256936</td>\n",
              "      <td>12.991043</td>\n",
              "      <td>169.851484</td>\n",
              "      <td>248.066191</td>\n",
              "      <td>1.982706</td>\n",
              "      <td>3.353865</td>\n",
              "      <td>4.358696</td>\n",
              "      <td>78858.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7240</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.33</td>\n",
              "      <td>N</td>\n",
              "      <td>246.596</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.2140</td>\n",
              "      <td>7.210000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.433</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sk</td>\n",
              "      <td>S</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.03429</td>\n",
              "      <td>MBA</td>\n",
              "      <td>0.226129</td>\n",
              "      <td>125.435355</td>\n",
              "      <td>10.594</td>\n",
              "      <td>1592.013769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 Vesta (A807 FA)</td>\n",
              "      <td>2.361418</td>\n",
              "      <td>0.088721</td>\n",
              "      <td>7.141771</td>\n",
              "      <td>103.810804</td>\n",
              "      <td>150.728541</td>\n",
              "      <td>2.151909</td>\n",
              "      <td>2.570926</td>\n",
              "      <td>3.628837</td>\n",
              "      <td>25372.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9397</td>\n",
              "      <td>2977.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>N</td>\n",
              "      <td>4 Vesta (A807 FA)</td>\n",
              "      <td>2.361418</td>\n",
              "      <td>0.088721</td>\n",
              "      <td>7.141771</td>\n",
              "      <td>103.810804</td>\n",
              "      <td>150.728541</td>\n",
              "      <td>2.151909</td>\n",
              "      <td>2.570926</td>\n",
              "      <td>3.628837</td>\n",
              "      <td>25372.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9397</td>\n",
              "      <td>2977.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>N</td>\n",
              "      <td>525.400</td>\n",
              "      <td>572.6 x 557.2 x 446.4</td>\n",
              "      <td>0.4228</td>\n",
              "      <td>5.342128</td>\n",
              "      <td>17.8000</td>\n",
              "      <td>0.782</td>\n",
              "      <td>0.492</td>\n",
              "      <td>NaN</td>\n",
              "      <td>V</td>\n",
              "      <td>V</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.13948</td>\n",
              "      <td>MBA</td>\n",
              "      <td>0.271609</td>\n",
              "      <td>95.861938</td>\n",
              "      <td>0.200</td>\n",
              "      <td>1325.432763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 Astraea (A845 XA)</td>\n",
              "      <td>2.574037</td>\n",
              "      <td>0.190913</td>\n",
              "      <td>5.367427</td>\n",
              "      <td>141.571025</td>\n",
              "      <td>358.648419</td>\n",
              "      <td>2.082619</td>\n",
              "      <td>3.065455</td>\n",
              "      <td>4.129814</td>\n",
              "      <td>63739.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3082</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.90</td>\n",
              "      <td>N</td>\n",
              "      <td>5 Astraea (A845 XA)</td>\n",
              "      <td>2.574037</td>\n",
              "      <td>0.190913</td>\n",
              "      <td>5.367427</td>\n",
              "      <td>141.571025</td>\n",
              "      <td>358.648419</td>\n",
              "      <td>2.082619</td>\n",
              "      <td>3.065455</td>\n",
              "      <td>4.129814</td>\n",
              "      <td>63739.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3082</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.90</td>\n",
              "      <td>N</td>\n",
              "      <td>106.699</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>16.806000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.411</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.09575</td>\n",
              "      <td>MBA</td>\n",
              "      <td>0.238661</td>\n",
              "      <td>17.846344</td>\n",
              "      <td>3.140</td>\n",
              "      <td>1508.414420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  full_name         a  ...  diameter_sigma          per\n",
              "0         1 Ceres (A801 AA)  2.769165  ...           0.200  1683.145703\n",
              "1        2 Pallas (A802 FA)  2.773841  ...          18.000  1687.410992\n",
              "2          3 Juno (A804 RA)  2.668285  ...          10.594  1592.013769\n",
              "3         4 Vesta (A807 FA)  2.361418  ...           0.200  1325.432763\n",
              "4       5 Astraea (A845 XA)  2.574037  ...           3.140  1508.414420\n",
              "\n",
              "[5 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9tylAunaiLN"
      },
      "source": [
        "# dropping any unncessary columns that are not required for the analysis\r\n",
        "dataset.drop(columns=['condition_code','n_obs_used','n_del_obs_used','n_dop_obs_used','condition_code.1','a.1','e.1','i.1','om.1','w.1','q.1','ad.1','per_y.1','data_arc.1','condition_code.1','n_obs_used.1','n_del_obs_used.1','n_dop_obs_used.1','H.1','extent','pha','neo','IR','full_name','full_name.1'],inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G_2nx6qxIxm"
      },
      "source": [
        "# comverting diameter and other object data types to numberic\r\n",
        "dataset['diameter']=pd.to_numeric(dataset['diameter'],errors='coerce')\r\n",
        "dataset['spec_B']=pd.to_numeric(dataset['spec_B'],errors='coerce')\r\n",
        "dataset['spec_T']=pd.to_numeric(dataset['spec_T'],errors='coerce')\r\n",
        "dataset['class']=pd.to_numeric(dataset['class'],errors='coerce')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVzL7oVYeyo0"
      },
      "source": [
        "# deleting any rows where diameter contains NaN values\r\n",
        "dataset = dataset[dataset['diameter'].notnull()]  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BynhPsrIM822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45014165-5619-463c-ff93-a8b34b26aca7"
      },
      "source": [
        "# checking for the percentage of NaN values in different columns\r\n",
        "for column in dataset.columns:\r\n",
        "    print(column, dataset[column].isna().sum()/dataset.shape[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a 0.0\n",
            "e 0.0\n",
            "i 0.0\n",
            "om 0.0\n",
            "w 0.0\n",
            "q 0.0\n",
            "ad 0.0\n",
            "per_y 0.0\n",
            "data_arc 0.0003856784727132481\n",
            "H 0.020055280581088897\n",
            "diameter 0.0\n",
            "albedo 0.008249233999700028\n",
            "rot_per 0.9028018826823223\n",
            "GM 0.9999000092848521\n",
            "BV 0.9928078107032918\n",
            "UB 0.9931006406547963\n",
            "spec_B 1.0\n",
            "spec_T 1.0\n",
            "G 0.9991500789212431\n",
            "moid 0.0\n",
            "class 1.0\n",
            "n 0.0\n",
            "ma 0.0\n",
            "diameter_sigma 0.0008999164363309121\n",
            "per 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLIjtfCMdyNg"
      },
      "source": [
        "# deleting any columns with more than 50% missing values\r\n",
        "tooMuchNa = dataset.columns[dataset.isna().sum()/dataset.shape[0] > 0.5]\r\n",
        "dataset = dataset.drop(tooMuchNa,axis=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR_IQPM0yhJw"
      },
      "source": [
        "# filling the missing values of H column with the mode of the entire column\r\n",
        "dataset.columns\r\n",
        "dataset=dataset.fillna(dataset['H'].mode())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibgx3sjVhp5I"
      },
      "source": [
        "# filling all the missing values from the remaining columns with the mean of the their respective columns\r\n",
        "dataset=dataset.fillna(dataset.mean())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kt6OdPfwAEh",
        "outputId": "1b0d0e3d-f1b9-484b-93ec-1d6708a280a1"
      },
      "source": [
        "dataset.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['a', 'e', 'i', 'om', 'w', 'q', 'ad', 'per_y', 'data_arc', 'H',\n",
              "       'diameter', 'albedo', 'moid', 'n', 'ma', 'diameter_sigma', 'per'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "4StajOy23Sa4",
        "outputId": "f204c17d-4eee-4a6d-f9d1-d860ca4c8cf1"
      },
      "source": [
        "# extracting the correlation matrix to determine what features that the diameter depends on\r\n",
        "datasetc=dataset.corr()\r\n",
        "datasetc"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>e</th>\n",
              "      <th>i</th>\n",
              "      <th>om</th>\n",
              "      <th>w</th>\n",
              "      <th>q</th>\n",
              "      <th>ad</th>\n",
              "      <th>per_y</th>\n",
              "      <th>data_arc</th>\n",
              "      <th>H</th>\n",
              "      <th>diameter</th>\n",
              "      <th>albedo</th>\n",
              "      <th>moid</th>\n",
              "      <th>n</th>\n",
              "      <th>ma</th>\n",
              "      <th>diameter_sigma</th>\n",
              "      <th>per</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.151783</td>\n",
              "      <td>-0.000351</td>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.373626</td>\n",
              "      <td>0.984952</td>\n",
              "      <td>0.938924</td>\n",
              "      <td>-0.026846</td>\n",
              "      <td>-0.130929</td>\n",
              "      <td>0.147350</td>\n",
              "      <td>-0.116290</td>\n",
              "      <td>0.377739</td>\n",
              "      <td>-0.279414</td>\n",
              "      <td>0.008972</td>\n",
              "      <td>0.201096</td>\n",
              "      <td>0.938924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>e</th>\n",
              "      <td>0.010390</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166829</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>0.011458</td>\n",
              "      <td>-0.540449</td>\n",
              "      <td>0.111654</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>-0.050940</td>\n",
              "      <td>0.253444</td>\n",
              "      <td>-0.060931</td>\n",
              "      <td>-0.021583</td>\n",
              "      <td>-0.510767</td>\n",
              "      <td>0.215717</td>\n",
              "      <td>-0.010984</td>\n",
              "      <td>-0.016787</td>\n",
              "      <td>0.044451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>0.151783</td>\n",
              "      <td>0.166829</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.012432</td>\n",
              "      <td>-0.005921</td>\n",
              "      <td>0.079005</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.096666</td>\n",
              "      <td>-0.188332</td>\n",
              "      <td>-0.044018</td>\n",
              "      <td>0.051276</td>\n",
              "      <td>-0.083805</td>\n",
              "      <td>0.120690</td>\n",
              "      <td>-0.103634</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>0.040714</td>\n",
              "      <td>0.096666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>om</th>\n",
              "      <td>-0.000351</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>-0.012432</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.106782</td>\n",
              "      <td>-0.003238</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>0.006615</td>\n",
              "      <td>0.004238</td>\n",
              "      <td>-0.001849</td>\n",
              "      <td>0.000462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>w</th>\n",
              "      <td>-0.002328</td>\n",
              "      <td>0.011458</td>\n",
              "      <td>-0.005921</td>\n",
              "      <td>-0.106782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.006230</td>\n",
              "      <td>-0.001295</td>\n",
              "      <td>-0.001636</td>\n",
              "      <td>-0.006138</td>\n",
              "      <td>-0.008628</td>\n",
              "      <td>0.003238</td>\n",
              "      <td>-0.002874</td>\n",
              "      <td>-0.006063</td>\n",
              "      <td>0.003440</td>\n",
              "      <td>-0.001147</td>\n",
              "      <td>0.001718</td>\n",
              "      <td>-0.001636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>q</th>\n",
              "      <td>0.373626</td>\n",
              "      <td>-0.540449</td>\n",
              "      <td>0.079005</td>\n",
              "      <td>-0.003238</td>\n",
              "      <td>-0.006230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.207692</td>\n",
              "      <td>0.111740</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>-0.379637</td>\n",
              "      <td>0.327952</td>\n",
              "      <td>-0.263136</td>\n",
              "      <td>0.996172</td>\n",
              "      <td>-0.688114</td>\n",
              "      <td>0.052792</td>\n",
              "      <td>0.360839</td>\n",
              "      <td>0.111740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ad</th>\n",
              "      <td>0.984952</td>\n",
              "      <td>0.111654</td>\n",
              "      <td>0.145345</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>-0.001295</td>\n",
              "      <td>0.207692</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969338</td>\n",
              "      <td>-0.023858</td>\n",
              "      <td>-0.067338</td>\n",
              "      <td>0.094286</td>\n",
              "      <td>-0.073607</td>\n",
              "      <td>0.212742</td>\n",
              "      <td>-0.166450</td>\n",
              "      <td>-0.000375</td>\n",
              "      <td>0.144837</td>\n",
              "      <td>0.969338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>per_y</th>\n",
              "      <td>0.938924</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>0.096666</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>-0.001636</td>\n",
              "      <td>0.111740</td>\n",
              "      <td>0.969338</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>-0.036027</td>\n",
              "      <td>0.050428</td>\n",
              "      <td>-0.020811</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>-0.059722</td>\n",
              "      <td>-0.005902</td>\n",
              "      <td>0.081167</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_arc</th>\n",
              "      <td>-0.026846</td>\n",
              "      <td>-0.050940</td>\n",
              "      <td>-0.188332</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>-0.006138</td>\n",
              "      <td>-0.023898</td>\n",
              "      <td>-0.023858</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.638377</td>\n",
              "      <td>0.488664</td>\n",
              "      <td>0.249080</td>\n",
              "      <td>-0.032765</td>\n",
              "      <td>0.053101</td>\n",
              "      <td>-0.023520</td>\n",
              "      <td>0.011496</td>\n",
              "      <td>-0.009247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>-0.130929</td>\n",
              "      <td>0.253444</td>\n",
              "      <td>-0.044018</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>-0.008628</td>\n",
              "      <td>-0.379637</td>\n",
              "      <td>-0.067338</td>\n",
              "      <td>-0.036027</td>\n",
              "      <td>-0.638377</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.563871</td>\n",
              "      <td>-0.221431</td>\n",
              "      <td>-0.373016</td>\n",
              "      <td>0.313851</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>-0.080778</td>\n",
              "      <td>-0.036027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diameter</th>\n",
              "      <td>0.147350</td>\n",
              "      <td>-0.060931</td>\n",
              "      <td>0.051276</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.003238</td>\n",
              "      <td>0.327952</td>\n",
              "      <td>0.094286</td>\n",
              "      <td>0.050428</td>\n",
              "      <td>0.488664</td>\n",
              "      <td>-0.563871</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.107479</td>\n",
              "      <td>0.331280</td>\n",
              "      <td>-0.190701</td>\n",
              "      <td>-0.002669</td>\n",
              "      <td>0.344263</td>\n",
              "      <td>0.050428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>albedo</th>\n",
              "      <td>-0.116290</td>\n",
              "      <td>-0.021583</td>\n",
              "      <td>-0.083805</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>-0.002874</td>\n",
              "      <td>-0.263136</td>\n",
              "      <td>-0.073607</td>\n",
              "      <td>-0.020811</td>\n",
              "      <td>0.249080</td>\n",
              "      <td>-0.221431</td>\n",
              "      <td>-0.107479</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.268175</td>\n",
              "      <td>0.338895</td>\n",
              "      <td>-0.006452</td>\n",
              "      <td>-0.084648</td>\n",
              "      <td>-0.020811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>moid</th>\n",
              "      <td>0.377739</td>\n",
              "      <td>-0.510767</td>\n",
              "      <td>0.120690</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.006063</td>\n",
              "      <td>0.996172</td>\n",
              "      <td>0.212742</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>-0.032765</td>\n",
              "      <td>-0.373016</td>\n",
              "      <td>0.331280</td>\n",
              "      <td>-0.268175</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.670370</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>0.365538</td>\n",
              "      <td>0.113728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <td>-0.279414</td>\n",
              "      <td>0.215717</td>\n",
              "      <td>-0.103634</td>\n",
              "      <td>0.006615</td>\n",
              "      <td>0.003440</td>\n",
              "      <td>-0.688114</td>\n",
              "      <td>-0.166450</td>\n",
              "      <td>-0.059722</td>\n",
              "      <td>0.053101</td>\n",
              "      <td>0.313851</td>\n",
              "      <td>-0.190701</td>\n",
              "      <td>0.338895</td>\n",
              "      <td>-0.670370</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.035915</td>\n",
              "      <td>-0.138668</td>\n",
              "      <td>-0.059722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ma</th>\n",
              "      <td>0.008972</td>\n",
              "      <td>-0.010984</td>\n",
              "      <td>0.013371</td>\n",
              "      <td>0.004238</td>\n",
              "      <td>-0.001147</td>\n",
              "      <td>0.052792</td>\n",
              "      <td>-0.000375</td>\n",
              "      <td>-0.005902</td>\n",
              "      <td>-0.023520</td>\n",
              "      <td>0.009083</td>\n",
              "      <td>-0.002669</td>\n",
              "      <td>-0.006452</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>-0.035915</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.023157</td>\n",
              "      <td>-0.005902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>diameter_sigma</th>\n",
              "      <td>0.201096</td>\n",
              "      <td>-0.016787</td>\n",
              "      <td>0.040714</td>\n",
              "      <td>-0.001849</td>\n",
              "      <td>0.001718</td>\n",
              "      <td>0.360839</td>\n",
              "      <td>0.144837</td>\n",
              "      <td>0.081167</td>\n",
              "      <td>0.011496</td>\n",
              "      <td>-0.080778</td>\n",
              "      <td>0.344263</td>\n",
              "      <td>-0.084648</td>\n",
              "      <td>0.365538</td>\n",
              "      <td>-0.138668</td>\n",
              "      <td>-0.023157</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.081167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>per</th>\n",
              "      <td>0.938924</td>\n",
              "      <td>0.044451</td>\n",
              "      <td>0.096666</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>-0.001636</td>\n",
              "      <td>0.111740</td>\n",
              "      <td>0.969338</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.009247</td>\n",
              "      <td>-0.036027</td>\n",
              "      <td>0.050428</td>\n",
              "      <td>-0.020811</td>\n",
              "      <td>0.113728</td>\n",
              "      <td>-0.059722</td>\n",
              "      <td>-0.005902</td>\n",
              "      <td>0.081167</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       a         e  ...  diameter_sigma       per\n",
              "a               1.000000  0.010390  ...        0.201096  0.938924\n",
              "e               0.010390  1.000000  ...       -0.016787  0.044451\n",
              "i               0.151783  0.166829  ...        0.040714  0.096666\n",
              "om             -0.000351  0.002089  ...       -0.001849  0.000462\n",
              "w              -0.002328  0.011458  ...        0.001718 -0.001636\n",
              "q               0.373626 -0.540449  ...        0.360839  0.111740\n",
              "ad              0.984952  0.111654  ...        0.144837  0.969338\n",
              "per_y           0.938924  0.044451  ...        0.081167  1.000000\n",
              "data_arc       -0.026846 -0.050940  ...        0.011496 -0.009247\n",
              "H              -0.130929  0.253444  ...       -0.080778 -0.036027\n",
              "diameter        0.147350 -0.060931  ...        0.344263  0.050428\n",
              "albedo         -0.116290 -0.021583  ...       -0.084648 -0.020811\n",
              "moid            0.377739 -0.510767  ...        0.365538  0.113728\n",
              "n              -0.279414  0.215717  ...       -0.138668 -0.059722\n",
              "ma              0.008972 -0.010984  ...       -0.023157 -0.005902\n",
              "diameter_sigma  0.201096 -0.016787  ...        1.000000  0.081167\n",
              "per             0.938924  0.044451  ...        0.081167  1.000000\n",
              "\n",
              "[17 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fhCtQeHRwNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa15c266-157c-4888-8c06-dc53db0cc4a0"
      },
      "source": [
        "# establishing the test dataset of features\n",
        "dataset1 = dataset[['a','i','q','ad','data_arc','moid','diameter_sigma']]\n",
        "print(dataset.head(20))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           a         e          i  ...          ma  diameter_sigma          per\n",
            "0   2.769165  0.076009  10.594067  ...   77.372098        0.200000  1683.145703\n",
            "1   2.773841  0.229972  34.832931  ...  144.975676       18.000000  1687.410992\n",
            "2   2.668285  0.256936  12.991043  ...  125.435355       10.594000  1592.013769\n",
            "3   2.361418  0.088721   7.141771  ...   95.861938        0.200000  1325.432763\n",
            "4   2.574037  0.190913   5.367427  ...   17.846344        3.140000  1508.414420\n",
            "5   2.424533  0.203219  14.739653  ...  190.686496        2.900000  1378.924506\n",
            "6   2.387375  0.230145   5.521598  ...  247.425812       10.000000  1347.347071\n",
            "7   2.201415  0.155833   5.889081  ...  315.318013        1.025000  1193.029574\n",
            "8   2.386189  0.123300   5.576494  ...   23.912205        0.485994  1346.343281\n",
            "9   3.142435  0.112117   3.831786  ...  222.850543        6.800000  2034.688644\n",
            "10  2.452390  0.099823   4.631823  ...   21.701825        1.008000  1402.757738\n",
            "11  2.333778  0.220439   8.372749  ...  243.834733        1.199000  1302.229877\n",
            "12  2.576064  0.085283  16.535532  ...  283.002605       50.084000  1510.196192\n",
            "13  2.586377  0.166250   9.121448  ...  259.795715        0.485994  1519.274373\n",
            "14  2.643690  0.186178  11.753619  ...   15.025609        2.234000  1570.052431\n",
            "15  2.922776  0.133531   3.096560  ...    7.048521       23.000000  1825.120868\n",
            "16  2.470965  0.133323   5.592661  ...   45.074275        2.027000  1418.725045\n",
            "17  2.295709  0.217505  10.131591  ...   20.462100        2.452000  1270.496864\n",
            "18  2.442857  0.156975   1.572666  ...  300.346238        0.485994  1394.586682\n",
            "19  2.409358  0.142494   0.708567  ...  222.460322        3.674000  1365.999296\n",
            "\n",
            "[20 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx9BWoN6pcDH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c4fa84a4-e92f-42e7-bc2d-95e5525c789b"
      },
      "source": [
        "# Establishing the test dataset for the output\n",
        "dataset2=dataset['diameter'].values\n",
        "dataset.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>e</th>\n",
              "      <th>i</th>\n",
              "      <th>om</th>\n",
              "      <th>w</th>\n",
              "      <th>q</th>\n",
              "      <th>ad</th>\n",
              "      <th>per_y</th>\n",
              "      <th>data_arc</th>\n",
              "      <th>H</th>\n",
              "      <th>diameter</th>\n",
              "      <th>albedo</th>\n",
              "      <th>moid</th>\n",
              "      <th>n</th>\n",
              "      <th>ma</th>\n",
              "      <th>diameter_sigma</th>\n",
              "      <th>per</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.769165</td>\n",
              "      <td>0.076009</td>\n",
              "      <td>10.594067</td>\n",
              "      <td>80.305531</td>\n",
              "      <td>73.597695</td>\n",
              "      <td>2.558684</td>\n",
              "      <td>2.979647</td>\n",
              "      <td>4.608202</td>\n",
              "      <td>9242.0</td>\n",
              "      <td>3.40</td>\n",
              "      <td>939.400</td>\n",
              "      <td>0.0900</td>\n",
              "      <td>1.59478</td>\n",
              "      <td>0.213885</td>\n",
              "      <td>77.372098</td>\n",
              "      <td>0.200</td>\n",
              "      <td>1683.145703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.773841</td>\n",
              "      <td>0.229972</td>\n",
              "      <td>34.832931</td>\n",
              "      <td>173.024741</td>\n",
              "      <td>310.202392</td>\n",
              "      <td>2.135935</td>\n",
              "      <td>3.411748</td>\n",
              "      <td>4.619880</td>\n",
              "      <td>78910.0</td>\n",
              "      <td>4.20</td>\n",
              "      <td>545.000</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>1.23429</td>\n",
              "      <td>0.213345</td>\n",
              "      <td>144.975676</td>\n",
              "      <td>18.000</td>\n",
              "      <td>1687.410992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.668285</td>\n",
              "      <td>0.256936</td>\n",
              "      <td>12.991043</td>\n",
              "      <td>169.851484</td>\n",
              "      <td>248.066191</td>\n",
              "      <td>1.982706</td>\n",
              "      <td>3.353865</td>\n",
              "      <td>4.358696</td>\n",
              "      <td>78858.0</td>\n",
              "      <td>5.33</td>\n",
              "      <td>246.596</td>\n",
              "      <td>0.2140</td>\n",
              "      <td>1.03429</td>\n",
              "      <td>0.226129</td>\n",
              "      <td>125.435355</td>\n",
              "      <td>10.594</td>\n",
              "      <td>1592.013769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.361418</td>\n",
              "      <td>0.088721</td>\n",
              "      <td>7.141771</td>\n",
              "      <td>103.810804</td>\n",
              "      <td>150.728541</td>\n",
              "      <td>2.151909</td>\n",
              "      <td>2.570926</td>\n",
              "      <td>3.628837</td>\n",
              "      <td>25372.0</td>\n",
              "      <td>3.00</td>\n",
              "      <td>525.400</td>\n",
              "      <td>0.4228</td>\n",
              "      <td>1.13948</td>\n",
              "      <td>0.271609</td>\n",
              "      <td>95.861938</td>\n",
              "      <td>0.200</td>\n",
              "      <td>1325.432763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.574037</td>\n",
              "      <td>0.190913</td>\n",
              "      <td>5.367427</td>\n",
              "      <td>141.571025</td>\n",
              "      <td>358.648419</td>\n",
              "      <td>2.082619</td>\n",
              "      <td>3.065455</td>\n",
              "      <td>4.129814</td>\n",
              "      <td>63739.0</td>\n",
              "      <td>6.90</td>\n",
              "      <td>106.699</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>1.09575</td>\n",
              "      <td>0.238661</td>\n",
              "      <td>17.846344</td>\n",
              "      <td>3.140</td>\n",
              "      <td>1508.414420</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          a         e          i  ...          ma  diameter_sigma          per\n",
              "0  2.769165  0.076009  10.594067  ...   77.372098           0.200  1683.145703\n",
              "1  2.773841  0.229972  34.832931  ...  144.975676          18.000  1687.410992\n",
              "2  2.668285  0.256936  12.991043  ...  125.435355          10.594  1592.013769\n",
              "3  2.361418  0.088721   7.141771  ...   95.861938           0.200  1325.432763\n",
              "4  2.574037  0.190913   5.367427  ...   17.846344           3.140  1508.414420\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcrdFR3rLHOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2912a48a-170c-4736-bea1-40af924e753c"
      },
      "source": [
        "#Scaling data\n",
        "scale_x = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_x=scale_x.fit_transform(dataset1)\n",
        "print(scaled_x.shape)\n",
        "dataset2=dataset2.reshape(-1, 1)\n",
        "scale_y = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_y=scale_y.fit_transform(dataset2)\n",
        "print(scaled_y.shape)\n",
        "print(scaled_x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(140013, 7)\n",
            "(140013, 1)\n",
            "[[0.00570697 0.06207417 0.06155793 ... 0.11710958 0.04051714 0.00142857]\n",
            " [0.00571942 0.20439407 0.05105128 ... 1.         0.03135834 0.12857143]\n",
            " [0.00543831 0.07614815 0.04724305 ... 0.99934101 0.02627704 0.07567143]\n",
            " ...\n",
            " [0.0054638  0.09806536 0.04831215 ... 0.08976162 0.02666169 0.00346429]\n",
            " [0.00576988 0.05069223 0.05655463 ... 0.04762448 0.03484691 0.00605714]\n",
            " [0.00561283 0.08861884 0.04893609 ... 0.06646897 0.02860504 0.00138571]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1GZazJU4bB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776da69d-4a20-440e-93af-3b38a3f59840"
      },
      "source": [
        "#Converting data into readable format of LSTM for previous 24 timesteps\n",
        "x=list()\n",
        "y=list()\n",
        "ws=100\n",
        "for i in range(len(dataset)-ws):\n",
        "  xx=list()\n",
        "  for j in range(0,ws-1,1):\n",
        "    value=scaled_x[i+j]\n",
        "    xx.append(value)\n",
        "  x.append(xx)\n",
        "  yy=scaled_y[i+ws-1]\n",
        "  y.append(yy)\n",
        "#Input of  LSTM \n",
        "x=np.array(x)\n",
        "#Output of LSTM\n",
        "y=np.array(y)\n",
        "print(x.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(139913, 99, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLPGpKY53VMo"
      },
      "source": [
        "#Splitting data into training 70% , testing 15% and validation 15%\n",
        "n_train_hours = 97931\n",
        "n_valid_hours = 118917\n",
        "\n",
        "train_x = x[:n_train_hours]\n",
        "test_x = x[n_valid_hours:]\n",
        "valid_x= x[n_train_hours:n_valid_hours]\n",
        "\n",
        "train_y = y[:n_train_hours]\n",
        "test_y = y[n_valid_hours:]\n",
        "valid_y= y[n_train_hours:n_valid_hours]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WAfASPpzmeG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d32dfc6a-2cce-42dc-dffc-189d30107c8c"
      },
      "source": [
        "#Declarationof model\n",
        "model = Sequential()\n",
        "#adding LSTM layer\n",
        "model.add(LSTM(64, input_shape=(9,7),kernel_initializer='truncated_normal'))\n",
        "model.add(Dense(1,activation='linear'))\n",
        "model.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "#Training the model\n",
        "history = model.fit(train_x, train_y, epochs=300, batch_size=25000, validation_data=(valid_x, valid_y), verbose=1, shuffle=False)\n",
        "#Plotting the loss graphs\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 9, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 7), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\"), but it was called on an input with incompatible shape (None, 99, 7).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 9, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 7), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\"), but it was called on an input with incompatible shape (None, 99, 7).\n",
            "4/4 [==============================] - ETA: 0s - loss: 0.0594WARNING:tensorflow:Model was constructed with shape (None, 9, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 7), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\"), but it was called on an input with incompatible shape (None, 99, 7).\n",
            "4/4 [==============================] - 2s 318ms/step - loss: 0.0603 - val_loss: 0.0173\n",
            "Epoch 2/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0081 - val_loss: 0.0055\n",
            "Epoch 3/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0106 - val_loss: 0.0135\n",
            "Epoch 4/300\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0171 - val_loss: 0.0179\n",
            "Epoch 5/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0140 - val_loss: 0.0080\n",
            "Epoch 6/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0087 - val_loss: 0.0050\n",
            "Epoch 7/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0054 - val_loss: 0.0018\n",
            "Epoch 8/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0049 - val_loss: 0.0035\n",
            "Epoch 9/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0051 - val_loss: 0.0019\n",
            "Epoch 10/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0043 - val_loss: 0.0022\n",
            "Epoch 11/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 12/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 13/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 14/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 15/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 16/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 17/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 18/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 19/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 20/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 21/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 22/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 23/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 24/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 25/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 26/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0042 - val_loss: 0.0014\n",
            "Epoch 27/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 28/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 29/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 30/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 31/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 32/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0045 - val_loss: 0.0022\n",
            "Epoch 33/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0047 - val_loss: 0.0024\n",
            "Epoch 34/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0045 - val_loss: 0.0023\n",
            "Epoch 35/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0042 - val_loss: 0.0021\n",
            "Epoch 36/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0044 - val_loss: 0.0016\n",
            "Epoch 37/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0053 - val_loss: 0.0042\n",
            "Epoch 38/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0066 - val_loss: 0.0025\n",
            "Epoch 39/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0046 - val_loss: 0.0026\n",
            "Epoch 40/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 41/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0047 - val_loss: 0.0016\n",
            "Epoch 42/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0041 - val_loss: 0.0019\n",
            "Epoch 43/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0044 - val_loss: 0.0016\n",
            "Epoch 44/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 45/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0041 - val_loss: 0.0017\n",
            "Epoch 46/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0041 - val_loss: 0.0018\n",
            "Epoch 47/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 48/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 49/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 50/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 51/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 52/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 53/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0042 - val_loss: 0.0020\n",
            "Epoch 54/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0042 - val_loss: 0.0021\n",
            "Epoch 55/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 56/300\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 57/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0043 - val_loss: 0.0022\n",
            "Epoch 58/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0047 - val_loss: 0.0014\n",
            "Epoch 59/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0044 - val_loss: 0.0031\n",
            "Epoch 60/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0051 - val_loss: 0.0028\n",
            "Epoch 61/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0048 - val_loss: 0.0025\n",
            "Epoch 62/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0050 - val_loss: 0.0016\n",
            "Epoch 63/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0047 - val_loss: 0.0018\n",
            "Epoch 64/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0040 - val_loss: 0.0017\n",
            "Epoch 65/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 66/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 67/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 68/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 69/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 70/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 71/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 72/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 73/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 74/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 75/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 76/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 77/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 78/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 79/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 80/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 81/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 82/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 83/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0042 - val_loss: 0.0023\n",
            "Epoch 84/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0044 - val_loss: 0.0023\n",
            "Epoch 85/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 86/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 87/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0047 - val_loss: 0.0015\n",
            "Epoch 88/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0043 - val_loss: 0.0021\n",
            "Epoch 89/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0043 - val_loss: 0.0023\n",
            "Epoch 90/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 91/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 92/300\n",
            "4/4 [==============================] - 1s 203ms/step - loss: 0.0042 - val_loss: 0.0014\n",
            "Epoch 93/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 94/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 95/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 96/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 97/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 98/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 99/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 100/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 101/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 102/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 103/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 104/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 105/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 106/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 107/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 108/300\n",
            "4/4 [==============================] - 1s 242ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 109/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 110/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0040 - val_loss: 0.0019\n",
            "Epoch 111/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 112/300\n",
            "4/4 [==============================] - 1s 220ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 113/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 114/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 115/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 116/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0040 - val_loss: 0.0018\n",
            "Epoch 117/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 118/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 119/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 120/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 121/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0040 - val_loss: 0.0019\n",
            "Epoch 122/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 123/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 124/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 125/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 126/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 127/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 128/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 129/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 130/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 131/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 132/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 133/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 134/300\n",
            "4/4 [==============================] - 1s 203ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 135/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 136/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 137/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 138/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 139/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 140/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 141/300\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 142/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 143/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 144/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 145/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 146/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 147/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 148/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 149/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 150/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 151/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 152/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 153/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 154/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 155/300\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 156/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 157/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 158/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 159/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 160/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 161/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 162/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 163/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 164/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 165/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 166/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 167/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 168/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 169/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 170/300\n",
            "4/4 [==============================] - 1s 245ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 171/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 172/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 173/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 174/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 175/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 176/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 177/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 178/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 179/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 180/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 181/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 182/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 183/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 184/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 185/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 186/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 187/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 188/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 189/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 190/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 191/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 192/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 193/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 194/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 195/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 196/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 197/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 198/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 199/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 200/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 201/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 202/300\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 203/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 204/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 205/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 206/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 207/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 208/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 209/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 210/300\n",
            "4/4 [==============================] - 1s 204ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 211/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 212/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 213/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 214/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 215/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 216/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 217/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 218/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 219/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 220/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 221/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 222/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 223/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 224/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 225/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 226/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 227/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 228/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 229/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 230/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 231/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 232/300\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 233/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 234/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 235/300\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 236/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 237/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 238/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 239/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 240/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 241/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 242/300\n",
            "4/4 [==============================] - 1s 206ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 243/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 244/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 245/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 246/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 247/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 248/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 249/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 250/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 251/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 252/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 253/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 254/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 255/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 256/300\n",
            "4/4 [==============================] - 1s 218ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 257/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 258/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 259/300\n",
            "4/4 [==============================] - 1s 216ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 260/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 261/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 262/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 263/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 264/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 265/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 266/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 267/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 268/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 269/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 270/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 271/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 272/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 273/300\n",
            "4/4 [==============================] - 1s 221ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 274/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 275/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 276/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 277/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 278/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 279/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 280/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 281/300\n",
            "4/4 [==============================] - 1s 214ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 282/300\n",
            "4/4 [==============================] - 1s 204ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 283/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 284/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 285/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 286/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 287/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 288/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 289/300\n",
            "4/4 [==============================] - 1s 205ms/step - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 290/300\n",
            "4/4 [==============================] - 1s 213ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 291/300\n",
            "4/4 [==============================] - 1s 208ms/step - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 292/300\n",
            "4/4 [==============================] - 1s 217ms/step - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 293/300\n",
            "4/4 [==============================] - 1s 209ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 294/300\n",
            "4/4 [==============================] - 1s 250ms/step - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 295/300\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 296/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 297/300\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 298/300\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 299/300\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 300/300\n",
            "4/4 [==============================] - 1s 203ms/step - loss: 0.0037 - val_loss: 0.0014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7lvqaR7n9XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b859c413-d34f-4c13-9a4b-8b5dbbdfc429"
      },
      "source": [
        "#Rescaling the data \n",
        "import sklearn.metrics as metrics\n",
        "yhat = model.predict(test_x)\n",
        "\n",
        "inv_yhat = scale_y.inverse_transform(yhat)\n",
        "\n",
        "inv_y = scale_y.inverse_transform(test_y)\n",
        "\n",
        "#Finding the accuracy with RMSE function\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 9, 7) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 7), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\"), but it was called on an input with incompatible shape (None, 99, 7).\n",
            "Test RMSE: 1.737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6An77X2WdsIk",
        "outputId": "aa8e488a-9d72-49f9-acdd-c2cb2b60e061"
      },
      "source": [
        "# Calculating nRMSE\r\n",
        "nrmse=rmse/np.mean(inv_y)\r\n",
        "nrmse"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5905229212637421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}